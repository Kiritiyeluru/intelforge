Codebase Structure is Crucial.
Coding
Okay as a senior dev, I seemed to have learned this lesson late with Claude Code.

If you want good results, find a way to structure your code base in a way that makes logical sense to Claude.

I'm a React Next.js dev, and the worst thing about this stack is that it's constantly changing. There's not "one" way to do things. Honestly I wish someone would just make an LLM friendly framework, but alas.

So previously I've been just pretty loose with the file structure, trying to use a combination of server actions, api routes, lib and utility files, without really a rhyme or reason as to which files go where and how they are structured. Lots of individual utility functions spread out everywhere. Consuming endpoints in react hooks, some context in tanstack query, some in local state, some in zustand. In short, a mess.

Anyways, lately I decided enough was enough, and decided to pull the trigger with oRPC. You may already be using tRPC, and I don't know the big difference between the two, but basically oRPC is similar, just that its built on OpenAPI standards (not openAI).

Front and back are fully typesafe, uses tanstack query to cache and fetch... but most importantly, groups everything logically.
Routers are where you define your input and output typesafe contracts. They then call out to services. Services will use 'repositories' to query the db. From there, you can built out any number of patterns. I've started using engines, processors, utilities, algos, strategies...
But the interesting thing is that you add this to the file name.
translation.strategy.ts
translation.router.ts
translation.repository.ts

etc.

This seems to give claude a ton of context just with the filename, and along with a few examples from the docs, it's been refactoring my entire app basically in one shot without bugs and perfect types. Green files in vscode.

Honestly, I'm super happy with it. Ctrl Clicking function names that bring you to the implementation right from the react frontend files is amazing, and clearly it makes sense for Claude.

Check it out: https://orpc.unnoq.com/docs/getting-started
--------------------------------------------
Run MCP Servers In Seconds With Docker
#
ai
#
docker
#
mcp
#
llm
Model Context Protocol (MCP) has taken the AI world by storm. It has become the de facto standard for how an AI Agent connect with tools, services, and data. As this is shaping up rapidly, working with different MCP servers, setting them up is still not an easy task, and it requires a learning curve. Docker has a track record of making developers’ lives easier to make, build and ship things faster and again it chimes in to the MCP space, bringing that same clarity, trust, and scalability. That’s exactly what Docker is doing with and introduction of Docker MCP Catalog and Docker MCP Toolkit after the Docker Model Runner (if you haven’t checked it out, here is the link).

In this blog, we will first under what Docker MCP Catalog and MCP Toolkit are. Then we will see step-by-step how we can use Docker MCP Toolkit using Docker Desktop to interact with various tools using MCP Clients offered by Claude, Cursor, etc.

What is Docker MCP Catalog?
Docker MCP Catalog is a trusted collection of MCP servers. Currently has verified tools from 100 verified (and the number keeps bumping while writing this) tools publishers like Stripe, Elastic, Grafana, etc. And the tools are it’s just like container images, that means like traditional pull mechanism, we can pull and use it (or use MCP toolkit for UI perks, more on that later) without any hassle to find and configure it manually.

mcp server list website

What is Docker MCP Toolkit?
With Docker MCP Toolkit, with a single click of a button from Docker Desktop, we can spin MCP servers in seconds and connect to our favourite client like Cluade, Cursor, Windsurf, Docker AI Agent, etc. The way it works is that a Gateway MCP Server is created and dynamically exposes enabled tools to compatible clients. This makes it so easy to manage all the tools in one place.

mcp server list docker desktop

Using Docker MCP Toolkit
Let’s now test Docker MCP Toolkit. Make sure you have the latest version of Docker Desktop. My current version is Docker Desktop (Mac) is 4.43.0 (196668). Once you open it, you will see the MCP Toolkit button on the sidebar. Initially, it was shipped as an extension; now it’s baked into the Docker Desktop itself.

mcp docker desktop catalog

Now let’s install/turn on some MCP servers like curl and Wikipedia. You can search and add it. It’s that simple. It’s really handy to add and remove when needed. No copying and pasting of manual config, and managing them.

mcp tookit tools

Now, let’s connect the Dockerized MCP servers to our MCP clients. I will be using Claude; you can use any according to your preference. We simply need to click on the Connect button, and it will automatically add the Docker configuration to Claude Desktop's MCP server config claude_desktop_config.json file. The same goes for other MCP Clients.

tool clients

Let’s open Claude and see it. It will be Settings > Developer > MCP_DOCKER. As you can see, it’s running, which means everything is correctly configured. If we click on the Edit Config button, we can see the config and how it works.

claude mcp <br>
![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/sefrakhw7nzvuybmeb77.png)config

Config:

{
   "mcpServers":{
      "MCP_DOCKER":{
         "command":"docker",
         "args":[
            "mcp",
            "gateway",
            "run"
         ]
      }
   }
}
Let’s close the config and open the chat screen on Claude. Now, click on the Search and Tools option to see all the MCP servers, for just, it’s just one, MCP_DOCKER,, having 10 tools. If you are not seeing it, completely close down Claude and re-open it, and it will start showing up.

!
cluade desktop

We can click on the arrow next to 10 to see all the available tools.

tools list

Now, let’s test it out.

To test curl, I will ask whether the website is up or not. When you enter the prompt, you might get a pop-up saying “Claude would like to use an external integration”; it is just to determine whether you want to use the MCP tools or not. You can either choose, always allow or allow once, depending on your preference.

curl test

Let’s now search for some history so that it uses the Wikipedia tool. As you can see, it is called both search_wikipedia and get_wikipedia and gives the result.
-----------------------------------------
These 9 MCP Servers Improve AI Context (Reduce 99% Code Errors)
Joe Njenga
Joe Njenga

Following
8 min read
·
4 days ago
545


8






If you are tired of AI coding hallucinations and endless loops, you are missing this — improved context!

Nearly all AI coding assistants keep hallucinating outdated APIs, forgetting the project context, and make the same mistakes over and over.

If you have spent hours debugging why your React component won’t render, only to discover your AI was using deprecated hooks from 2022, you know this pain.

AI coding tools have context limitations that are costing developers real time and sanity.

They forget previous conversations, reference outdated documentation, and lack awareness of your project’s full scope.

But here’s what changed everything for me.

I discovered that MCP servers can transform your AI coding agent from a context-confused into a project-aware coding partner.

After testing dozens of MCP servers specifically for context improvement, I found 9 that eliminate the most common AI coding errors.

I’ve been running these MCP servers for months, and my debugging time has dropped dramatically.

Here are the MCP servers that now help improve context and code faster with fewer errors

1. Context7 MCP
Context7 is a game-changer for eliminating outdated documentation errors.

It pulls the latest library docs directly into your AI prompts, stopping those frustrating moments when your assistant references deprecated APIs or non-existent functions.

I’ve been using Context7, and it’s eliminated 90% of my documentation-related debugging sessions.

Key Features
Fetches the current, version-specific documentation from the source
Retrieves accurate code examples for modern frameworks
Works with just a simple “use context7” in your prompt
Supports multiple programming languages and frameworks
Updates automatically as libraries evolve
Errors It Prevents
Deprecated API usage
Incorrect function signatures
Outdated syntax recommendations
Missing required parameters
Wrong import statements
Best Use Cases
Building projects with rapidly evolving frameworks
Learning new libraries without constant tab-switching
Ensuring code examples work with your current dependencies
Getting accurate syntax for specific package versions
Git Link: @upstash/context7-mcp

2. Memory Bank MCP Server
Memory Bank MCP creates persistent memory for your AI assistant across all coding sessions.

This eliminates the repetitive explanations and context rebuilding that wastes so much development time.

Your AI remembers your coding patterns, project decisions, and architectural choices from previous sessions.

Key Features
Centralized memory bank service with remote access
Multi-project support with complete isolation
Path validation and security controls
Persistent storage across sessions and restarts
Project-specific context retention
Errors It Prevents
Duplicate function creation
Inconsistent coding patterns
Repeated architectural mistakes
Lost project context between sessions
Forgetting previous decisions and constraints
Best Use Cases
Long-term project development
Team collaboration with shared context
Maintaining coding standards across sessions
Building on previous architectural decisions
Avoiding repeated explanations of project structure
Git Link: @alioshr/memory-bank-mcp

3. Knowledge Graph Memory Server
Knowledge Graph Memory takes context awareness to the next level by understanding relationships between different parts of your project.

Instead of treating each file as isolated, it maps how your components, functions, and modules connect and depend on each other.

This prevents cascade errors where changing one piece breaks something seemingly unrelated.

I use this constantly for refactoring tasks where understanding component relationships is crucial.

Key Features
Persistent memory using local knowledge graph
Relationship mapping between code components
Lightweight context retention across sessions
Dependency tracking and impact analysis
Cross-file reference understanding
Errors It Prevents
Breaking changes in dependent components
Missing import updates during refactoring
Circular dependency creation
Unused code accumulation
Incomplete feature implementations
Best Use Cases
Large codebase refactoring
Component relationship analysis
Dependency impact assessment
Architectural decision tracking
Cross-module feature development
Git Link: modelcontextprotocol/servers — memory

4. Filesystem MCP Server
Filesystem MCP gives your AI accurate, real-time access to your project structure and files.

This eliminates the guesswork about file locations, directory structures, and project organization that leads to broken imports and missing files.

Your AI can now see exactly what exists in your project before making suggestions or generating code.

It’s incredibly powerful for preventing file-related errors.

Key Features
Read and write files with simple commands
Create, list, and delete directories accurately
Move files and directories without breaking references
Search files using pattern matching
Get detailed file metadata and structure
Restricted directory access for security
Errors It Prevents
Incorrect file path references
Missing import statements
Wrong directory structures
Duplicate file creation
Broken relative path imports
Best Use Cases
Managing project files during development
Bulk file operations and reorganization
Searching for specific code patterns across projects
Retrieving accurate file details for debugging
Maintaining a consistent project structure
Git Link: @modelcontextprotocol/server-filesystem

5. GitMCP
GitMCP transforms your AI assistant into a git-aware coding partner that understands your repository history, branches, and version control context.

Instead of suggesting changes that conflict with recent commits or ignore your branching strategy, your AI now works with full repository awareness.

This prevents those frustrating moments when your AI generates code that breaks existing functionality or ignores recent changes made by team members.

I’ve found GitMCP particularly valuable when working on feature branches where context about recent changes is crucial.

Key Features
Repository and file operations with full git context
Issue tracking and management integration
User and contributor awareness
Dynamic toolset for repos, issues, and users
Branch and commit history understanding
Merge conflict prevention through context awareness
Errors It Prevents
Code conflicts with recent commits
Overwriting teammate changes
Breaking existing functionality
Ignoring branch-specific requirements
Missing repository context in suggestions
Best Use Cases
Team collaboration with multiple contributors
Feature branch development
Code review and conflict resolution
Issue-driven development workflows
Repository-wide refactoring projects
Git Link: https://gitmcp.io/

6. Obsidian-MCP
Obsidian-MCP connects your AI assistant to your Obsidian knowledge base, bringing your notes, documentation, and project insights directly into coding sessions.

This creates a bridge between your thinking process and your coding, ensuring your AI understands what you’re building and how it fits into your broader project goals.

Your project documentation, architecture decisions, and learning notes become part of your AI’s context.

This prevents creating features that don’t align with your documented requirements or architectural decisions.

Key Features
Direct access to Obsidian vault notes
Markdown document integration
Linked note relationship understanding
Tag and category awareness
Project documentation context
Decision history tracking
Errors It Prevents
Building features against documented requirements
Ignoring architectural decisions
Missing project constraints and goals
Inconsistent implementation patterns
Lost context from previous planning sessions
Best Use Cases
Documentation-driven development
Maintaining architectural consistency
Learning from previous project notes
Requirement-aligned feature development
Knowledge-based coding decisions
Git Link: Obsidian-MCP integration

7. Tavily MCP
Tavily MCP adds AI-powered search capabilities that go beyond basic web search to find developer-specific, contextually relevant information.

When your AI needs current information about libraries, frameworks, or solutions to specific coding problems, Tavily provides intelligent search results.

This prevents outdated solutions and ensures your AI has access to the latest best practices and problem-solving approaches.

Key Features
AI-powered search with developer context
Current library and framework information
Best practice and solution discovery
Technical documentation search
Problem-specific result filtering
Errors It Prevents
Using outdated solutions and patterns
Missing current best practices
Implementing known problematic approaches
Reinventing solutions that already exist
Following deprecated recommendations
Best Use Cases
Researching new libraries and frameworks
Finding solutions to specific coding problems
Staying current with best practices
Discovering alternative approaches
Technical problem-solving research
Git Link: Tavily MCP

8. Sequential Thinking MCP
Sequential Thinking MCP gives structured problem-solving to your AI assistant, breaking complex coding tasks into logical, manageable steps.

AI now thinks through problems systematically, considering dependencies, edge cases, and the order of implementation.

This dramatically reduces errors that come from incomplete analysis or rushed implementations.

I use this for any complex feature that involves multiple components or has non-obvious implementation challenges.

Key Features
Breaks complex tasks into manageable steps
Supports branching logic and decision trees
Allows thought revision and refinement
Ideal for planning and analysis
Dependency identification and ordering
Risk assessment and mitigation planning
Errors It Prevents
Incomplete feature implementations
Missing edge case handling
Poor implementation order is causing conflicts
Overlooked dependencies and requirements
Rushed solutions without proper analysis
Best Use Cases
Complex feature planning and implementation
System architecture decisions
Debugging complex multi-component issues
Refactoring large codebases
Risk assessment for major changes
Git Link: Sequential Thinking MCP server

9. Fetch MCP Server
Fetch MCP Server gives your AI assistant the ability to retrieve and process web content directly, converting HTML documentation, tutorials, and resources into usable context.

This means your AI can pull the latest information from official documentation sites, GitHub repositories, and technical resources in real-time.

No more outdated information or missing context about external dependencies and services.

Key Features
Retrieves web content and converts HTML to markdown
Supports chunked reading through the start_index parameter
Handles content truncation with customizable max_length
Raw content option when needed
Real-time documentation access
External resource integration
Errors It Prevents
Outdated external API information
Missing current documentation details
Incorrect integration patterns
Stale third-party service information
Incomplete external dependency understanding
Best Use Cases
Researching external APIs and services
Accessing current documentation while coding
Integrating with third-party services
Following current implementation patterns
Staying updated with framework changes
Git Link: mcp-server-fetch

------------------------------------------------------------
Promptify: A Python Library for LLM Prompt Management
Code Coup
Code Coup

Follow
6 min read
·
Jun 16, 2025
94


3





About a year ago, I was knee-deep in a project, trying to extract medical terms from patient records for a healthcare startup.

As a tech content writer, I’ve seen my fair share of coding tools, but wrangling NLP tasks with raw LLMs like GPT was a headache —Until I discovered Promptify.

This Python library, with its prompter, LLM integration, and pipeline, turned hours of prompt tweaking into a few lines of code.

In this article, I’ll share why Promptify feels like a breath of fresh air for anyone tackling NLP, explained simply with code examples.



What’s Promptify, Anyway?
You’ve got a chunk of text, say a medical report or a story snippet, and you need to pull out key details, classify topics, or generate questions. Normally, you’d spend ages crafting the perfect prompt for an LLM like GPT or PaLM.

Promptify changes that.

It’s a Python library that pairs a prompter (for crafting prompts), an LLM (like OpenAI’s models), and a pipeline to streamline NLP tasks.

From diagnosing medical conditions to generating quiz questions based on Alice in Wonderland, Promptify makes it feel effortless — not because you’re cheating, but because it’s smart coding.

Why I love it:

Dead Simple: Pre-built templates (or custom ones) mean you don’t need to be a prompt wizard.

Flexible: Handles tasks like named entity recognition (NER), classification, or question generation.

Works with What You’ve Got: Supports Python 3.7+, OpenAI 0.25+, and multiple LLMs.

Let’s get it set up and dive into some examples that sold me on it.

Setting Up Promptify
Installing Promptify is as easy as it gets. I ran this on my trusty Python 3.8 setup:

pip3 install promptify
Want the bleeding-edge version? Grab it from GitHub:

pip3 install git+https://github.com/promptslab/Promptify.git
With that done, you’re ready to play. You’ll need an API key for your LLM (I used OpenAI’s), but more on that later.

My Promptify Experiments
When I first tried Promptify, I was skeptical — could it really simplify my NLP struggles? I tested it with three tasks: extracting entities from a medical record, classifying health conditions, and generating questions from a story. Here’s what happened.

Example 1: Pulling Medical Entities (NER)
Back at that healthcare startup, I had a messy patient record to parse:

The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection.

I needed to extract ages, conditions, and symptoms. Here’s how Promptify nailed it:

from promptify import Prompter, OpenAI, Pipeline

# The patient record
sentence = """The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection"""
# Set up OpenAI (swap in your API key)
model = OpenAI("your_api_key_here")
# Pick the NER template
prompter = Prompter('ner.jinja')
# Create the pipeline
pipe = Pipeline(prompter, model)
# Run it
result = pipe.fit(sentence, domain="medical", labels=None)
# Check the results
print(result)
What I Got:

[
    {"E": "93-year-old", "T": "Age"},
    {"E": "chronic right hip pain", "T": "Medical Condition"},
    {"E": "osteoporosis", "T": "Medical Condition"},
    {"E": "hypertension", "T": "Medical Condition"},
    {"E": "depression", "T": "Medical Condition"},
    {"E": "chronic atrial fibrillation", "T": "Medical Condition"},
    {"E": "severe nausea and vomiting", "T": "Symptom"},
    {"E": "urinary tract infection", "T": "Medical Condition"},
    {"Branch": "Internal Medicine", "Group": "Geriatrics"}
]
This blew my mind. In minutes, I had a clean list of entities, plus the bonus of categorizing the case under Internal Medicine and Geriatrics. It saved me hours of manual tagging for the startup’s database.

Example 2: Classifying Medical Conditions
Next, I wanted to label the conditions in that same record and group them by medical domain. Here’s the code I used:

from promptify import OpenAI, Prompter

# Same patient record
sentence = """The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection"""
# Initialize the model
model = OpenAI("your_api_key_here")
# Set up the prompter
nlp_prompter = Prompter(model)
# Run multilabel classification
result = nlp_prompter.fit('multilabel_classification.jinja', domain='medical', text_input=sentence)
# See what came out
print(result)4
Output:

[
    {
        '1': 'Medicine',
        '2': 'Osteoporosis',
        '3': 'Hypertension',
        '4': 'Depression',
        '5': 'Atrial fibrillation',
        '6': 'Nausea and vomiting',
        '7': 'Urinary tract infection',
        'branch': 'Health',
        'group': 'Clinical medicine',
        'main class': 'Health'
    }
]
This was a lifesaver for organizing data. Each condition got a clear label, and the broader “Health” and “Clinical medicine” tags helped the team sort records for analysis. Honestly, I was grinning like a kid when I saw how clean this output was.

Example 3: Generating Story Questions
To mix things up, I tried Promptify on a creative task. I’m a bit of a book nerd, so I fed it a line from Alice in Wonderland:

The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.

I wanted questions for a book club quiz. Here’s what I ran:

from promptify import OpenAI, Prompter
# The story snippet
sentence = """The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well."""
# Set up the model
model = OpenAI("your_api_key_here")
# Pick the question-answer template
nlp_prompter = Prompter(model)
# Generate questions
result = nlp_prompter.fit('qa_gen.jinja', domain='story_writing', text_input=sentence)
# Check the output
print(result)
Output:

[
    {'A': 'Alice found herself falling down a very deep well.', 'Q': 'What happened when Alice went down the rabbit-hole?'},
    {'A': 'Very deep.', 'Q': 'How deep was the well?'},
    {'A': 'No, she did not have a moment to think.', 'Q': 'Did Alice have time to think about stopping herself?'},
    {'A': 'It went straight on like a tunnel.', 'Q': 'What direction did the rabbit-hole go?'},
    {'A': 'No, she did not expect it.', 'Q': 'Did Alice expect to fall down a well?'}
]
I used these questions for a local book club, and they sparked a great discussion! Promptify turned a single sentence into a mini-quiz without me breaking a sweat.

Why Promptify Won Me Over
Look, I’ve written about tech tools for years, and I’m picky. Promptify hooked me because it’s like having a smart assistant who handles the boring stuff. Here’s why it’s worth your time:

Saves Brainpower: A few lines of code replace hours of prompt engineering.

Adapts to You: Use built-in templates or tweak your own for niche tasks.

Versatile as Heck: Medical data? Creative writing? It’s got you covered.

Plays Nice with LLMs: Swap between OpenAI, Hugging Face, or others without a fuss.

My only hiccup was grabbing an API key — OpenAI’s signup was quick, but don’t forget to check your LLM provider’s docs for that.
---------------------------------------------------------
The Full MCP Blueprint: Building a Full-Fledged MCP Workflow using Tools, Resources, and Prompts
Model context protocol crash course—Part 4.

Avi Chawla
Akshay Pachaar
Avi Chawla, Akshay Pachaar

Recap
Introduction
Resources
Intuition
What can resources represent?
URI-based resource identification
Resource types
Resource discovery mechanisms
Application-controlled access pattern
👉
Hey! This is a member-only post. But it looks like you are from India 🇮🇳. Join today by visiting this membership page for relief pricing of 50% off on your subscription, FOREVER.
Recap
Before we dive into Part 4 of the MCP crash course, let’s briefly recap what we covered in the previous part of this course.


In Part 3, we built a custom MCP client from scratch, integrated an LLM into it (which acted as the brain), and understood the full lifecycle of the model context protocol in action.


We also explored the client-server interaction model through practical implementations, observed how tool discovery and execution are handled dynamically, and contrasted MCP’s design with traditional function calling and API-based approaches.


We concluded Part 3 with some "try out yourself" style exercises to reinforce practical learning, while our discussion emphasized how MCP streamlines integration through its decoupled and modular architecture.

The hands-on walkthrough in Part 3 not only demystified MCP as a protocol but also highlighted its core strengths, like scalability, extensibility, and seamless tool orchestration.

By learning how tools are registered, discovered, and executed without tight API coupling, we saw how MCP allows developers to build adaptable and maintainable AI systems with ease.

If you haven’t explored Part 3 yet, we highly recommend doing so first since it lays the essential groundwork for what follows in this part. You can read it below:

The Full MCP Blueprint: Building a Custom MCP Client from Scratch
Model context protocol crash course—Part 3.

Daily Dose of Data Science
Avi Chawla

Introduction
Until now, our focus has primarily been on tools.

However, tools, prompts, and resources form the three core capabilities of the MCP framework.

While we introduced resources and prompts briefly in Part 2, this part will deep-dive into their mechanics, distinctions, and implementation.

We now shift gears to explore resources and prompts in detail and bring clarity to the key ideas around resources and prompts, like how they differ from tools, how to implement them, and how they enable richer, more contextual interactions when used in coordination.


By the end of this part, you'll have a practical and intuitive understanding of:

What exactly are resources and prompts in MCP
Implementing resources and prompts server-side
How tools, resources, and prompts differ from each other
Using resources and prompts inside the Claude Desktop
A full-fledged real-world use case powered by coordination across tools, prompts, and resources
Every concept will be explained through clear examples and walkthroughs to develop a solid understanding.

Let’s begin!

Resources
Resources are one of the fundamental primitives of the model context protocol (MCP).

While tools are about doing (executing actions), resources are about knowledge.

They expose contextual data to language models, allowing them to reason without causing any side effects.

👉
In programming and systems, a "side effect" refers to any observable change in system state beyond simply returning a value. This includes actions like writing to a file, updating a database, making an API call, modifying external systems or resources, etc.
Intuition
Resources are read-only interfaces that expose data as structured, contextual information.

They act as intelligent knowledge bases or reference libraries, allowing models to access and reason about information.

Think of resources as a well-organized library: they provide access to books (data) that can be read and referenced, but the books themselves cannot be altered through the reading process.

👉
There's a common misconception that resources mean data. However, it’s important to remember that a resource is not the data itself, but rather an interface that provides access to data in a structured and controlled manner.
What can resources represent?
Resources can represent various types of information:

A file's contents
An API's cached response
A database snapshot
An extracted snippet from a document
System logs, configurations, or documentation
Since resources should not be used to execute or modify data, they offer a safe and predictable way to supply context into AI workflows, especially when dealing with static or semi-static knowledge.

👉
If the underlying data changes, the updated content won’t automatically propagate into the model’s context. The resource must be reloaded into context to reflect the new state.
URI-based resource identification
Resources are identified through unique URIs (uniform resource identifiers) following a structured format:


The protocol and path structure are entirely specific to the MCP server implementation, allowing servers to create custom URI schemes that fit their specific use cases.

For example:


Resource types
Resources can contain two distinct types of content: text or binary data. This gives us text resources and binary resources.

Let's understand them!

Text resources
Text resources contain UTF-8 encoded text data, suitable for:


Source code files
Configuration files
Log files
JSON/XML data
Plain text documents
Binary resources
Binary resources contain raw binary data encoded in base64, suitable for:


Images and graphics
PDF documents
Audio and video files
Any non-text file formats
Resource discovery mechanisms
MCP provides two complementary approaches for resource discovery:

Direct resources
In this mechanism, an MCP server can expose concrete resources through the resources/list endpoint, which also provides metadata including URI, human-readable name, optional description, MIME type, and size information.



This approach works well for known, static resources that are always available.

👉
By static resources we refer to resources having fixed URIs.
Resource templates
For dynamic content, servers can expose URI templates like file://{path}, following RFC 6570 standards.


👉
RFC 6570 is a document published by the Internet Engineering Task Force (IETF) that defines the syntax and rules for URI Templates.
Resource templates are particularly powerful for scalable implementations.

Instead of registering thousands of individual files, a server can expose a single template that covers entire families of resources, dramatically reducing complexity while maintaining full functionality.

👉
It's okay if you don't understand this. Everything will become clear once we dive into the implementation.
Application-controlled access pattern
A crucial aspect of MCP resources is their application-controlled access pattern.

Unlike model-controlled primitives such as tools, resources require explicit client-side management.

To elaborate, when tools are invoked, the LLM returns the required tool call. The client, after permission from the user, invokes the tools that reside in the MCP server and receives the response. This depicts that the model is in control and the model decides what it is to be invoked:


But with resources, the client application must explicitly fetch and manage the data from the resources before providing it to the LLM, without the LLM initiating any action itself. This shows that the application is in the driver's seat for resources.

This design choice provides several important benefits
---------------------------------------------------------
