Great — I’ll search for Python-based frameworks or repositories that implement a full semantic crawler pipeline. This will include content discovery, async fetching, extraction, semantic relevance scoring (via sentence-transformers or LLMs), and dual-format output (Markdown + YAML and vector DBs like Qdrant).

I’ll also prioritize tools with optional features like tagging, scoring, CLI integration, and strong async performance. I’ll get back to you shortly with the most promising results.


# Semantic Web Crawling & Filtering Frameworks (Python)

Several open-source Python projects now combine web crawling with AI-driven filtering. They typically fetch pages (often via headless browsers), extract clean text (Markdown or JSON), embed the content with an AI model, and only retain items that score highly for a given topic. Examples include:

* **Crawl4AI** – An LLM-friendly crawler/scraper that outputs “smart, concise Markdown” optimized for RAG (retrieval-augmented generation). Crawl4AI uses Playwright or stealth HTTP fetchers to collect pages, then applies heuristic and LLM-based extraction to trim out ads and boilerplate.  It can build an embedding index (“Web Embedding Index”) for fast similarity search on the crawled content. Crawl4AI is actively maintained and even includes a CLI and web UI for testing strategies.
* **ScrapeGraphAI** – A Python scraping library that uses an LLM-driven pipeline to extract structured data. You specify *what* information you want, and the library uses “chain-of-thought” LLM calls plus a graph-based workflow to navigate pages and pull out the desired fields. It integrates with frameworks like LangChain or LlamaIndex for larger workflows. In other words, ScrapeGraphAI can turn a website or local HTML into JSON/CSV output by having an LLM “reason” about which links to follow and what to extract.
* **Firecrawl (Mendable)** – An open-source web crawling API with AI features. Firecrawl provides endpoints for *Scrape* (single-page) and *Crawl* (deep site crawl) modes, returning cleaned Markdown or JSON for each page. For example, its Scrape endpoint fetches one URL and returns Markdown+metadata, while its Crawl endpoint traverses all links on a site (up to a limit) and returns a list of page contents. Firecrawl also supports a “Map” operation (collect all site URLs) and even web *search*, leveraging LLMs to extract structured data from pages. It handles proxies, JS rendering, screenshots, and transforms any site into “LLM-ready” data (markdown, structured JSON, images, etc.).
* **eGet (vishwajeetdabholkar/eGet-Crawler-for-ai)** – A high-performance scraping framework aimed at AI workloads. eGet uses FastAPI + Selenium/Playwright to fetch pages (including heavy JavaScript), then automatically detects the main content and converts it to cleaned Markdown. It features smart HTML cleaning, PDF support, and link discovery. In short, eGet “transforms complex websites into AI-ready content” by outputting clean Markdown and JSON-LD metadata suitable for downstream embedding or RAG. This makes it ideal for building large knowledge bases (e.g. feeding an embedding index) from dynamic sites.
* **FastAPI + Postgres Vector Scraper** – A demo project that ties it all together in a web API. It uses FastAPI to accept URLs and trigger a Playwright crawl, then uses a sentence-transformers model to embed the scraped text and stores both text and vectors in PostgreSQL (with pgvector). The API provides endpoints to scrape single or batch URLs and to query the stored content by semantic similarity. Key features include batch (async) scraping, API key auth, and deduplication (it skips storing pages too similar to existing content). This illustrates a full pipeline: crawl → extract → embed → vector DB, all in Python.
* **Qdrant “page-search” example** – Qdrant’s example crawler/search service shows another pipeline. Its Python crawler (`site_search/crawl.py`) scrapes a documentation site into JSONL abstracts. It then encodes each abstract into vectors (using HuggingFace’s all-MiniLM-L6-v2 model) and uploads them to a Qdrant instance for fast similarity search. The provided service exposes a `/search` API that takes a query and returns semantically relevant pages. In short, this project crawls site docs, runs sentence embeddings, and uses a vector DB (Qdrant) to filter and retrieve relevant content.
* **AgentRAG (Followb1ind1y/AgentRAG)** – An end-to-end example of an AI agent pipeline. AgentRAG fetches URLs from a given sitemap (filtering them by keyword), then scrapes and chunks each page’s text. It generates OpenAI embeddings (`text-embedding-ada-002`) for each chunk and stores them in a Pinecone vector DB. The system then performs hybrid retrieval (cosine search + MMR re-ranking) to answer queries. The README explicitly outlines steps: “Web Crawling extracts and filters sitemap URLs” and “Vector Processing chunks content, generates OpenAI embeddings, and stores them in Pinecone”. AgentRAG even includes a FastAPI and Discord interface, illustrating how a crawled, filtered corpus can power RAG chatbots.
* **Other examples** – For instance, Sstobo’s *Site-Sn33k* repo shows a simpler scraper+vectorizer. It uses `wget` to recursively download a site, cleans the HTML with BeautifulSoup into text files, then chunks the text (JSONL) and embeds it with OpenAI’s embedding API, pushing vectors to Pinecone. This chain – “rip webpages, clean data, and vectorize it” – shows how one can build a semantic cache of a site (including PDFs) for later AI queries.

Each of the above combines crawling/discovery with AI-based filtering or ranking. They support async or headless fetch (Playwright/selenium), content extraction (Markdown/JSON), embeddings (sentence-transformers or OpenAI), and relevance scoring (cosine similarity, BM25, or LLMs) to decide what to keep. Relevant results can then be stored in Markdown files or in vector databases (Qdrant, Pinecone, pgvector, etc.) for fast semantic lookup.

**Sources:** These tools and demos are documented on GitHub or related blogs. For example, Crawl4AI’s GitHub readme highlights its LLM-optimized Markdown output, ScrapeGraphAI’s repo explains its LLM-guided pipeline, and Qdrant’s page-search docs show their crawl→encode→search flow. Each cited source provides details on the features above.
