# ğŸ”Œ IntelForge MCP Servers

**Model Context Protocol (MCP) servers for enhancing IntelForge's AI-powered knowledge extraction capabilities.**

## ğŸ“ Directory Structure

```
mcp_servers/
â”œâ”€â”€ README.md                    # This file - comprehensive MCP server guide
â”œâ”€â”€ config/                      # MCP server configurations
â”‚   â”œâ”€â”€ claude_desktop.json     # Claude Code configuration
â”‚   â”œâ”€â”€ server_configs.yaml     # Server-specific settings
â”‚   â””â”€â”€ recommended.json        # Recommended server configurations
â”œâ”€â”€ scripts/                     # Installation and management scripts
â”‚   â”œâ”€â”€ install.sh              # Master installation script
â”‚   â”œâ”€â”€ install_scraping.sh     # Scraping-focused servers
â”‚   â”œâ”€â”€ install_productivity.sh # Productivity servers
â”‚   â”œâ”€â”€ start_servers.sh        # Start MCP servers
â”‚   â””â”€â”€ stop_servers.sh         # Stop MCP servers
â”œâ”€â”€ servers/                     # MCP server implementations
â”‚   â”œâ”€â”€ scraping/               # Web scraping & crawling servers
â”‚   â”œâ”€â”€ productivity/           # Development & productivity servers
â”‚   â”œâ”€â”€ data/                   # Data processing servers
â”‚   â””â”€â”€ community/              # Community-contributed servers
â”œâ”€â”€ logs/                       # Server logs and debugging
â””â”€â”€ docs/                       # Documentation and guides
    â”œâ”€â”€ installation.md         # Installation instructions
    â”œâ”€â”€ configuration.md        # Configuration guide
    â””â”€â”€ troubleshooting.md      # Common issues and solutions
```

## ğŸ¯ Server Categories

### ğŸ•·ï¸ Scraping & Crawling Servers

**Comprehensive web data extraction infrastructure for IntelForge.**

#### ğŸ­ Enterprise-Grade Servers
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[AgentQL](#agentql)** | Local/git | ğŸ”¥ Advanced | Structured data from unstructured web |
| **[Apify](#apify)** | Local/git | ğŸ’¼ Enterprise | 3,000+ pre-built scraping tools |
| **[Browserbase](#browserbase)** | Remote/API | â˜ï¸ Cloud | Cloud browser automation |
| **[Firecrawl](#firecrawl)** | Local/git | â­ Premium | Production-grade scraping |
| **[Oxylabs](#oxylabs)** | Remote/API | ğŸ’¼ Enterprise | Professional proxy scraping |

#### ğŸ”§ Core Automation Servers  
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Puppeteer](#puppeteer)** | Local/npx | âœ… Recommended | JavaScript-heavy trading sites |
| **[Playwright](#playwright)** | Local/git | âœ… Recommended | Modern browser automation |
| **[Hyperbrowser](#hyperbrowser)** | Remote/API | ğŸš€ Advanced | Scalable browser automation |

#### ğŸ›¡ï¸ Anti-Detection Servers
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Rquest](#rquest)** | Local/git | ğŸ”¥ Advanced | TLS fingerprint bypass |
| **[Scrapling Fetch](#scrapling-fetch)** | Local/git | ğŸ”¥ Advanced | Bot-protected sites |
| **[RAG Web Browser](#rag-web-browser)** | Local/git | ğŸ“„ Specialized | Web search + Markdown conversion |

#### ğŸŒ Basic & Utility Servers
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Fetch](#fetch)** | Local/npx | âœ… Basic | Simple HTTP requests |
| **[ScreenshotOne](#screenshotone)** | Remote/API | ğŸ“¸ Utility | Website screenshots |

### ğŸ“„ Content Processing & AI-Ready Servers

**Essential for converting scraped content to AI-readable, chunked format.**

#### ğŸ”„ Document Conversion & Chunking
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Vectorize](#vectorize)** | Local/git | â­ Premium | Anything-to-Markdown + chunking |
| **[Pandoc](#pandoc)** | Local/git | âœ… Recommended | Universal document conversion |
| **[Markdownify](#markdownify)** | Local/git | âœ… Recommended | Multi-format to Markdown |
| **[Unstructured](#unstructured)** | Local/git | ğŸ’¼ Enterprise | Enterprise document processing |

#### ğŸ§  RAG & Knowledge Management  
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Needle](#needle)** | Local/git | â­ Premium | Production-ready RAG |
| **[Chroma](#chroma)** | Local/git | ğŸ”¥ Advanced | Vector database + embeddings |
| **[Basic Memory](#basic-memory)** | Local/git | âœ… Recommended | Semantic graph from Markdown |
| **[Inkeep](#inkeep)** | Remote/API | ğŸ¤– AI | RAG search over content |

### ğŸ“Š Data Processing Servers

**For organizing and analyzing scraped content.**

| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[SQLite](#sqlite)** | Local/npx | âœ… Recommended | Structured data storage |
| **[Memory](#memory)** | Local/npx | âœ… Active | Knowledge graph management |
| **[Filesystem](#filesystem)** | Local/npx | âœ… Active | File operations |
| **[PostgreSQL](#postgresql)** | Local/npx | ğŸ”§ Optional | Advanced database operations |

### ğŸš€ Productivity Servers

**For development and workflow enhancement.**

| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[GitHub](#github)** | Remote/HTTP | âœ… Active | Repository management |
| **[Git](#git)** | Local/uvx | ğŸ”§ Optional | Local git operations |
| **[Everything](#everything)** | Local/npx | ğŸ”§ Optional | Reference/testing |

### ğŸ” Search & Discovery Servers

**AI-powered content discovery and research infrastructure.**

#### ğŸ¤– AI-Powered Search
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Exa](#exa)** | Remote/API | ğŸ¤– AI | Search engine made for AIs |
| **[Tavily](#tavily)** | Remote/API | â­ Premium | AI agent search + extract |
| **[Perplexity](#perplexity)** | Remote/API | ğŸ¤– AI | Real-time web research |

#### ğŸ” Traditional Search
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Brave Search](#brave-search)** | Local/npx | âœ… Recommended | Privacy-focused web search |
| **[Kagi Search](#kagi-search)** | Remote/API | ğŸ”’ Privacy | Premium privacy search |
| **[SearXNG](#searxng)** | Local/docker | ğŸ”’ Privacy | Self-hosted metasearch |

#### ğŸ“Š Specialized Search
| Server | Transport | Status | Use Case |
|--------|-----------|---------|----------|
| **[Search1API](#search1api)** | Remote/API | ğŸ”§ Utility | Unified search, crawling, sitemaps |

## ğŸš€ Quick Start

### 1. Install Essential Servers

```bash
# Navigate to MCP servers directory
cd /home/kiriti/alpha_projects/intelforge/mcp_servers

# Install scraping essentials
./scripts/install_scraping.sh

# Install content processing (AI-ready chunking)
./scripts/install_content_processing.sh

# Install productivity tools
./scripts/install_productivity.sh
```

### 2. Configure Claude Code

```bash
# Copy recommended configuration
cp config/recommended.json ~/.claude/config.json

# Or manually add to Claude Code settings
```

### 3. Start Servers

```bash
# Start all configured servers
./scripts/start_servers.sh

# Or start specific category
./scripts/start_servers.sh scraping
```

## ğŸ“‹ Detailed Server Specifications

### ğŸ•·ï¸ Scraping Servers

#### Puppeteer
```bash
# Installation
npx @modelcontextprotocol/server-puppeteer

# Configuration
{
  "puppeteer": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-puppeteer"]
  }
}
```

**Features:**
- âœ… JavaScript execution
- âœ… Dynamic content rendering
- âœ… Screenshot capabilities
- âœ… Form interaction
- âš¡ Medium performance

**Best for:** Financial sites, SPAs, dynamic trading platforms

#### Firecrawl
```bash
# Installation
git clone https://github.com/mendableai/firecrawl-mcp-server
cd firecrawl-mcp-server && npm install

# Configuration
{
  "firecrawl": {
    "command": "node",
    "args": ["path/to/firecrawl-mcp-server/index.js"],
    "env": {
      "FIRECRAWL_API_KEY": "your-api-key"
    }
  }
}
```

**Features:**
- â­ Production-grade reliability
- âœ… Anti-detection measures
- âœ… PDF processing
- âœ… Rate limiting
- âœ… Structured data extraction
- ğŸš€ High performance

**Best for:** Large-scale scraping, enterprise sites, document processing

#### Playwright
```bash
# Installation
git clone https://github.com/executeautomation/mcp-playwright
cd mcp-playwright && npm install

# Configuration
{
  "playwright": {
    "command": "node",
    "args": ["path/to/mcp-playwright/index.js"]
  }
}
```

**Features:**
- ğŸ”¥ Modern browser automation
- âœ… Multi-browser support (Chrome, Firefox, Safari)
- âœ… Mobile emulation
- âœ… Network interception
- âœ… Advanced debugging
- âš¡ High performance

**Best for:** Modern web apps, testing, complex interactions

#### Brave Search
```bash
# Installation
npx @modelcontextprotocol/server-brave-search

# Configuration
{
  "brave-search": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-brave-search"],
    "env": {
      "BRAVE_SEARCH_API_KEY": "your-api-key"
    }
  }
}
```

**Features:**
- ğŸ” Web search integration
- âœ… Real-time results
- âœ… Privacy-focused
- âœ… Multiple result types
- âš¡ Fast response

**Best for:** Content discovery, research, trend analysis

#### AgentQL
```bash
# Installation
git clone https://github.com/tinyfish-io/agentql-mcp
cd agentql-mcp && npm install

# Configuration
{
  "agentql": {
    "command": "node",
    "args": ["path/to/agentql-mcp/index.js"],
    "env": {
      "AGENTQL_API_KEY": "your-api-key"
    }
  }
}
```

**Features:**
- ğŸ§  AI-powered data extraction
- âœ… Natural language queries
- âœ… Structured data output
- âœ… Dynamic web content handling
- ğŸš€ High accuracy

**Best for:** Complex data extraction, AI-driven scraping, structured output

#### Apify
```bash
# Installation
git clone https://github.com/apify/actors-mcp-server
cd actors-mcp-server && npm install

# Configuration
{
  "apify": {
    "command": "node",
    "args": ["path/to/actors-mcp-server/index.js"],
    "env": {
      "APIFY_TOKEN": "your-apify-token"
    }
  }
}
```

**Features:**
- ğŸ­ 3,000+ pre-built scrapers
- âœ… Social media scraping
- âœ… E-commerce data extraction
- âœ… Search engine scraping
- âœ… Map data extraction
- âœ… Cloud-based processing

**Best for:** Large-scale scraping, social media, e-commerce, enterprise data

#### Browserbase
```bash
# Configuration (API-based)
{
  "browserbase": {
    "command": "node",
    "args": ["path/to/browserbase-mcp/index.js"],
    "env": {
      "BROWSERBASE_API_KEY": "your-api-key",
      "BROWSERBASE_PROJECT_ID": "your-project-id"
    }
  }
}
```

**Features:**
- â˜ï¸ Cloud browser instances
- âœ… Scalable automation
- âœ… No infrastructure management
- âœ… Advanced debugging
- âœ… Session persistence
- ğŸš€ Enterprise reliability

**Best for:** Cloud automation, scalable scraping, enterprise deployments

#### Hyperbrowser
```bash
# Installation
git clone https://github.com/hyperbrowserai/mcp
cd mcp && npm install

# Configuration
{
  "hyperbrowser": {
    "command": "node",
    "args": ["path/to/hyperbrowser-mcp/index.js"],
    "env": {
      "HYPERBROWSER_API_KEY": "your-api-key"
    }
  }
}
```

**Features:**
- ğŸš€ Next-generation platform
- âœ… Effortless browser automation
- âœ… Scalable architecture
- âœ… AI agent optimization
- âœ… Advanced anti-detection

**Best for:** Modern automation, AI agents, scalable workflows

#### Rquest
```bash
# Installation
git clone https://github.com/xxxbrian/mcp-rquest
cd mcp-rquest && pip install -r requirements.txt

# Configuration
{
  "rquest": {
    "command": "python",
    "args": ["path/to/mcp-rquest/server.py"]
  }
}
```

**Features:**
- ğŸ›¡ï¸ Realistic browser fingerprints
- âœ… TLS/JA3/JA4 fingerprinting
- âœ… Anti-bot bypass
- âœ… Proxy support
- âœ… High success rate

**Best for:** Bot-protected sites, anti-detection, security research

#### RAG Web Browser
```bash
# Installation
git clone https://github.com/apify/mcp-server-rag-web-browser
cd mcp-server-rag-web-browser && npm install

# Configuration
{
  "rag-web-browser": {
    "command": "node",
    "args": ["path/to/rag-web-browser/index.js"],
    "env": {
      "APIFY_TOKEN": "your-apify-token"
    }
  }
}
```

**Features:**
- ğŸ“„ Web search and scraping
- âœ… Markdown output
- âœ… Content summarization
- âœ… URL processing
- ğŸ¤– RAG-optimized

**Best for:** Content research, document processing, RAG workflows

#### ScreenshotOne
```bash
# Configuration (API-based)
{
  "screenshotone": {
    "command": "node",
    "args": ["path/to/screenshotone-mcp/index.js"],
    "env": {
      "SCREENSHOTONE_API_KEY": "your-api-key"
    }
  }
}
```

**Features:**
- ğŸ“¸ Website screenshots
- âœ… Multiple formats
- âœ… Custom dimensions
- âœ… Full page capture
- âš¡ Fast rendering

**Best for:** Visual documentation, website monitoring, UI testing

### ğŸ“Š Data Servers

#### SQLite
```bash
# Installation
npx @modelcontextprotocol/server-sqlite

# Configuration
{
  "sqlite": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-sqlite", "/path/to/database.db"]
  }
}
```

**Features:**
- ğŸ—„ï¸ Local database storage
- âœ… SQL query interface
- âœ… ACID compliance
- âœ… Business intelligence
- âš¡ Fast queries

**Best for:** Structured data storage, analytics, reporting

#### Memory
```bash
# Installation (already active)
npx @modelcontextprotocol/server-memory

# Configuration
{
  "memory": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-memory"]
  }
}
```

**Features:**
- ğŸ§  Knowledge graph storage
- âœ… Persistent memory
- âœ… Relationship mapping
- âœ… Semantic search
- ğŸ“š Cross-session continuity

**Best for:** Knowledge management, relationship tracking, AI memory

### ğŸ” Search & Discovery Servers

#### Exa
```bash
# Configuration (API-based)
{
  "exa": {
    "command": "node",
    "args": ["path/to/exa-mcp-server/index.js"],
    "env": {
      "EXA_API_KEY": "your-exa-api-key"
    }
  }
}
```

**Features:**
- ğŸ¤– AI-optimized search
- âœ… Real-time web data
- âœ… Semantic understanding
- âœ… Developer-friendly API
- ğŸš€ Fast response times

**Best for:** AI agents, semantic search, real-time research

#### Tavily
```bash
# Installation
git clone https://github.com/tavily-ai/tavily-mcp
cd tavily-mcp && npm install

# Configuration
{
  "tavily": {
    "command": "node",
    "args": ["path/to/tavily-mcp/index.js"],
    "env": {
      "TAVILY_API_KEY": "your-tavily-api-key"
    }
  }
}
```

**Features:**
- ğŸ” AI agent search engine
- âœ… Search + extract capabilities
- âœ… Real-time results
- âœ… Content summarization
- ğŸ¤– Agent-optimized

**Best for:** AI agents, comprehensive research, content extraction

#### Kagi Search
```bash
# Installation
git clone https://github.com/kagisearch/kagimcp
cd kagimcp && npm install

# Configuration
{
  "kagi-search": {
    "command": "node",
    "args": ["path/to/kagimcp/index.js"],
    "env": {
      "KAGI_API_KEY": "your-kagi-api-key"
    }
  }
}
```

**Features:**
- ğŸ”’ Privacy-focused search
- âœ… Ad-free results
- âœ… High-quality ranking
- âœ… Fast response
- ğŸ’° Premium service

**Best for:** Privacy-conscious research, high-quality results, professional use

#### SearXNG
```bash
# Installation (Docker-based)
docker run -d --name searxng \
  -p 8080:8080 \
  searxng/searxng

# Configuration
{
  "searxng": {
    "command": "node",
    "args": ["path/to/searxng-mcp/index.js"],
    "env": {
      "SEARXNG_URL": "http://localhost:8080"
    }
  }
}
```

**Features:**
- ğŸ”’ Self-hosted metasearch
- âœ… Privacy protection
- âœ… No tracking
- âœ… Multiple search engines
- ğŸ†“ Open source

**Best for:** Privacy, self-hosting, aggregated search results

#### Perplexity
```bash
# Installation
git clone https://github.com/ppl-ai/modelcontextprotocol
cd modelcontextprotocol && npm install

# Configuration
{
  "perplexity": {
    "command": "node",
    "args": ["path/to/perplexity-mcp/index.js"],
    "env": {
      "PERPLEXITY_API_KEY": "your-perplexity-api-key"
    }
  }
}
```

**Features:**
- ğŸ¤– AI-powered research
- âœ… Real-time web data
- âœ… Source citations
- âœ… Conversational interface
- ğŸ“š Knowledge synthesis

**Best for:** Real-time research, cited information, conversational queries

#### Search1API
```bash
# Configuration (API-based)
{
  "search1api": {
    "command": "node",
    "args": ["path/to/search1api-mcp/index.js"],
    "env": {
      "SEARCH1API_KEY": "your-api-key"
    }
  }
}
```

**Features:**
- ğŸ”— Unified API
- âœ… Search capabilities
- âœ… Web crawling
- âœ… Sitemap generation
- âš¡ Single interface

**Best for:** Unified search operations, API consolidation, development efficiency

### ğŸ“„ Content Processing & AI-Ready Servers

#### Vectorize
```bash
# Installation
git clone https://github.com/vectorize-io/vectorize-mcp-server
cd vectorize-mcp-server && npm install

# Configuration
{
  "vectorize": {
    "command": "node",
    "args": ["path/to/vectorize-mcp-server/index.js"],
    "env": {
      "VECTORIZE_API_KEY": "your-vectorize-api-key"
    }
  }
}
```

**Features:**
- â­ Advanced retrieval capabilities
- ğŸ“„ Anything-to-Markdown file extraction
- âœ‚ï¸ Automatic text chunking
- ğŸ” Private Deep Research
- ğŸ¤– AI-optimized processing

**Best for:** Automatic content chunking, document conversion, RAG preparation

#### Pandoc
```bash
# Installation
git clone https://github.com/vivekVells/mcp-pandoc
cd mcp-pandoc && npm install

# Configuration
{
  "pandoc": {
    "command": "node",
    "args": ["path/to/mcp-pandoc/index.js"]
  }
}
```

**Features:**
- ğŸ”„ Universal document conversion
- ğŸ“‹ Markdown, HTML, PDF, DOCX, CSV support
- âš¡ Industry-standard Pandoc engine
- âœ… High-quality conversions
- ğŸ› ï¸ Extensive format support

**Best for:** Universal document conversion, format standardization

#### Markdownify
```bash
# Installation
git clone https://github.com/zcaceres/mcp-markdownify-server
cd mcp-markdownify-server && npm install

# Configuration
{
  "markdownify": {
    "command": "node",
    "args": ["path/to/markdownify-server/index.js"]
  }
}
```

**Features:**
- ğŸ“„ Convert almost anything to Markdown
- ğŸ¥ PPTX, HTML, PDF, YouTube Transcripts
- ğŸ¤– AI-optimized Markdown output
- âœ… Clean, structured conversion
- âš¡ Fast processing

**Best for:** Markdown standardization, YouTube/video content processing

#### Unstructured
```bash
# Installation
git clone https://github.com/Unstructured-IO/UNS-MCP
cd UNS-MCP && pip install -r requirements.txt

# Configuration
{
  "unstructured": {
    "command": "python",
    "args": ["path/to/unstructured-mcp/server.py"],
    "env": {
      "UNSTRUCTURED_API_KEY": "your-unstructured-api-key"
    }
  }
}
```

**Features:**
- ğŸ­ Enterprise-grade document processing
- ğŸ“Š Unstructured data workflows
- âœ‚ï¸ Advanced chunking strategies
- ğŸ”§ Customizable processing pipelines
- ğŸš€ Production-ready scaling

**Best for:** Enterprise document processing, complex data workflows

#### Needle
```bash
# Installation
git clone https://github.com/needle-ai/needle-mcp
cd needle-mcp && npm install

# Configuration
{
  "needle": {
    "command": "node",
    "args": ["path/to/needle-mcp/index.js"],
    "env": {
      "NEEDLE_API_KEY": "your-needle-api-key"
    }
  }
}
```

**Features:**
- ğŸ¯ Production-ready RAG out of the box
- ğŸ” Search and retrieve from your documents
- ğŸ“š Document indexing and management
- ğŸ¤– AI-optimized retrieval
- âš¡ Fast semantic search

**Best for:** Production RAG, document search, knowledge retrieval

#### Chroma
```bash
# Installation
git clone https://github.com/chroma-core/chroma-mcp
cd chroma-mcp && pip install -r requirements.txt

# Configuration
{
  "chroma": {
    "command": "python",
    "args": ["path/to/chroma-mcp/server.py"]
  }
}
```

**Features:**
- ğŸ§  Embeddings and vector search
- ğŸ“„ Document storage
- ğŸ” Full-text search capabilities
- ğŸ¤– AI application database
- ğŸ“Š Semantic similarity search

**Best for:** Vector database, embeddings storage, semantic search

#### Basic Memory
```bash
# Installation
git clone https://github.com/basicmachines-co/basic-memory
cd basic-memory && npm install

# Configuration
{
  "basic-memory": {
    "command": "node",
    "args": ["path/to/basic-memory/index.js"]
  }
}
```

**Features:**
- ğŸ§  Local-first knowledge management
- ğŸ“Š Semantic graph from Markdown files
- ğŸ’¾ Persistent memory across conversations
- ğŸ”— Relationship mapping
- ğŸ“š Cross-session continuity

**Best for:** Knowledge graphs, relationship tracking, persistent AI memory

#### Inkeep
```bash
# Configuration (API-based)
{
  "inkeep": {
    "command": "node",
    "args": ["path/to/inkeep-mcp/index.js"],
    "env": {
      "INKEEP_API_KEY": "your-inkeep-api-key"
    }
  }
}
```

**Features:**
- ğŸ” RAG search over your content
- ğŸ¤– AI-powered content retrieval
- ğŸ“š Knowledge base integration
- âš¡ Fast semantic search
- ğŸ¯ Precise content matching

**Best for:** Content search, knowledge base queries, RAG-powered retrieval

## ğŸ”§ Installation Scripts

### All-in-One Installation

```bash
#!/bin/bash
# install.sh - Master installation script

echo "ğŸš€ Installing IntelForge MCP Servers..."

# Create directories
mkdir -p servers/{scraping,productivity,data,community}
mkdir -p logs config

# Install essential scraping servers
echo "ğŸ“¥ Installing scraping servers..."
./scripts/install_scraping.sh

# Install productivity servers
echo "âš¡ Installing productivity servers..."
./scripts/install_productivity.sh

# Install content processing servers
echo "ğŸ“„ Installing content processing servers..."
./scripts/install_content_processing.sh

echo "âœ… Installation complete!"
echo "ğŸ“– Next steps:"
echo "1. Configure Claude Code: cp config/recommended.json ~/.claude/config.json"
echo "2. Start servers: ./scripts/start_servers.sh"
echo "3. Check status: ./scripts/status.sh"
```

### Scraping-Focused Installation

```bash
#!/bin/bash
# install_scraping.sh - Install scraping and crawling servers

echo "ğŸ•·ï¸ Installing scraping servers for IntelForge..."

# Official servers via npx
echo "ğŸ“¦ Installing official servers..."
echo "âœ… Puppeteer - JavaScript browser automation"
echo "âœ… Fetch - Basic HTTP requests"
echo "âœ… Brave Search - Web search integration"

# Community servers via git
echo "ğŸ“¥ Installing community servers..."

# Firecrawl (production scraping)
if [ ! -d "servers/scraping/firecrawl-mcp-server" ]; then
    echo "ğŸ“¥ Installing Firecrawl..."
    cd servers/scraping
    git clone https://github.com/mendableai/firecrawl-mcp-server
    cd firecrawl-mcp-server && npm install
    cd ../../..
fi

# Playwright (modern automation)
if [ ! -d "servers/scraping/mcp-playwright" ]; then
    echo "ğŸ“¥ Installing Playwright..."
    cd servers/scraping
    git clone https://github.com/executeautomation/mcp-playwright
    cd mcp-playwright && npm install
    cd ../../..
fi

# Scrapling Fetch (anti-bot protection)
if [ ! -d "servers/scraping/scrapling-fetch-mcp" ]; then
    echo "ğŸ“¥ Installing Scrapling Fetch..."
    cd servers/scraping
    git clone https://github.com/cyberchitta/scrapling-fetch-mcp
    cd scrapling-fetch-mcp && pip install -r requirements.txt
    cd ../../..
fi

echo "âœ… Scraping servers installed!"
```

### Content Processing Installation

```bash
#!/bin/bash
# install_content_processing.sh - Install content processing and AI-ready servers

echo "ğŸ“„ Installing content processing servers for IntelForge..."

# Document conversion servers
echo "ğŸ“¥ Installing document conversion servers..."

# Vectorize (chunking + conversion)
if [ ! -d "servers/data/vectorize-mcp-server" ]; then
    echo "ğŸ“¥ Installing Vectorize..."
    cd servers/data
    git clone https://github.com/vectorize-io/vectorize-mcp-server
    cd vectorize-mcp-server && npm install
    cd ../../..
fi

# Pandoc (universal conversion)
if [ ! -d "servers/data/mcp-pandoc" ]; then
    echo "ğŸ“¥ Installing Pandoc..."
    cd servers/data
    git clone https://github.com/vivekVells/mcp-pandoc
    cd mcp-pandoc && npm install
    cd ../../..
fi

# Markdownify (markdown conversion)
if [ ! -d "servers/data/mcp-markdownify-server" ]; then
    echo "ğŸ“¥ Installing Markdownify..."
    cd servers/data
    git clone https://github.com/zcaceres/mcp-markdownify-server
    cd mcp-markdownify-server && npm install
    cd ../../..
fi

# Unstructured (enterprise processing)
if [ ! -d "servers/data/UNS-MCP" ]; then
    echo "ğŸ“¥ Installing Unstructured..."
    cd servers/data
    git clone https://github.com/Unstructured-IO/UNS-MCP
    cd UNS-MCP && pip install -r requirements.txt
    cd ../../..
fi

# RAG & knowledge management servers
echo "ğŸ§  Installing RAG and knowledge management servers..."

# Needle (production RAG)
if [ ! -d "servers/data/needle-mcp" ]; then
    echo "ğŸ“¥ Installing Needle..."
    cd servers/data
    git clone https://github.com/needle-ai/needle-mcp
    cd needle-mcp && npm install
    cd ../../..
fi

# Chroma (vector database)
if [ ! -d "servers/data/chroma-mcp" ]; then
    echo "ğŸ“¥ Installing Chroma..."
    cd servers/data
    git clone https://github.com/chroma-core/chroma-mcp
    cd chroma-mcp && pip install -r requirements.txt
    cd ../../..
fi

# Basic Memory (knowledge graphs)
if [ ! -d "servers/data/basic-memory" ]; then
    echo "ğŸ“¥ Installing Basic Memory..."
    cd servers/data
    git clone https://github.com/basicmachines-co/basic-memory
    cd basic-memory && npm install
    cd ../../..
fi

echo "âœ… Content processing servers installed!"
echo "ğŸ“– Key capabilities added:"
echo "  ğŸ”„ Document conversion: Vectorize, Pandoc, Markdownify"
echo "  ğŸ§  RAG processing: Needle, Chroma, Basic Memory"
echo "  ğŸ“Š Enterprise workflows: Unstructured"
echo "ğŸ¯ Perfect for: Article chunking, AI-ready content processing"
```

## ğŸ“ Configuration Templates

### Claude Code Configuration

#### Essential Configuration
```json
{
  "mcpServers": {
    "github": {
      "command": "claude",
      "args": ["mcp", "add", "--transport", "http", "github", "https://api.githubcopilot.com/mcp/"]
    },
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/kiriti/alpha_projects/intelforge"]
    },
    "memory": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-memory"]
    },
    "puppeteer": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-puppeteer"]
    },
    "sqlite": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sqlite", "/home/kiriti/alpha_projects/intelforge/vault/data/scraped_data.db"]
    },
    "brave-search": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-brave-search"],
      "env": {
        "BRAVE_SEARCH_API_KEY": "your-api-key-here"
      }
    }
  }
}
```

#### Enterprise Scraping Configuration
```json
{
  "mcpServers": {
    "agentql": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/scraping/agentql-mcp/index.js"],
      "env": {
        "AGENTQL_API_KEY": "your-agentql-api-key"
      }
    },
    "apify": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/scraping/actors-mcp-server/index.js"],
      "env": {
        "APIFY_TOKEN": "your-apify-token"
      }
    },
    "firecrawl": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/scraping/firecrawl-mcp-server/index.js"],
      "env": {
        "FIRECRAWL_API_KEY": "your-firecrawl-api-key"
      }
    },
    "playwright": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/scraping/mcp-playwright/index.js"]
    },
    "rquest": {
      "command": "python",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/scraping/mcp-rquest/server.py"]
    }
  }
}
```

#### AI Search Configuration
```json
{
  "mcpServers": {
    "exa": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/community/exa-mcp-server/index.js"],
      "env": {
        "EXA_API_KEY": "your-exa-api-key"
      }
    },
    "tavily": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/community/tavily-mcp/index.js"],
      "env": {
        "TAVILY_API_KEY": "your-tavily-api-key"
      }
    },
    "perplexity": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/community/perplexity-mcp/index.js"],
      "env": {
        "PERPLEXITY_API_KEY": "your-perplexity-api-key"
      }
    },
    "kagi-search": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/community/kagimcp/index.js"],
      "env": {
        "KAGI_API_KEY": "your-kagi-api-key"
      }
    }
  }
}
```

#### Content Processing Configuration
```json
{
  "mcpServers": {
    "vectorize": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/data/vectorize-mcp-server/index.js"],
      "env": {
        "VECTORIZE_API_KEY": "your-vectorize-api-key"
      }
    },
    "pandoc": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/data/mcp-pandoc/index.js"]
    },
    "markdownify": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/data/mcp-markdownify-server/index.js"]
    },
    "unstructured": {
      "command": "python",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/data/UNS-MCP/server.py"],
      "env": {
        "UNSTRUCTURED_API_KEY": "your-unstructured-api-key"
      }
    },
    "needle": {
      "command": "node",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/data/needle-mcp/index.js"],
      "env": {
        "NEEDLE_API_KEY": "your-needle-api-key"
      }
    },
    "chroma": {
      "command": "python",
      "args": ["/home/kiriti/alpha_projects/intelforge/mcp_servers/servers/data/chroma-mcp/server.py"]
    }
  }
}
```

### Server-Specific Settings

```yaml
# server_configs.yaml
scraping:
  # Core automation
  puppeteer:
    headless: true
    timeout: 30000
    viewport:
      width: 1920
      height: 1080
  
  playwright:
    browsers: ["chromium", "firefox"]
    mobile_emulation: false
    network_idle: 500
  
  # Enterprise-grade
  firecrawl:
    rate_limit: 10
    max_pages: 100
    formats: ["markdown", "html"]
    
  agentql:
    timeout: 45000
    max_retries: 3
    output_format: "structured"
    
  apify:
    max_concurrent: 5
    timeout: 60000
    storage_days: 7
    
  browserbase:
    session_timeout: 300
    keep_alive: true
    debug_mode: false
  
  # Anti-detection
  rquest:
    proxy_rotation: true
    fingerprint_randomization: true
    success_threshold: 0.95
    
  scrapling_fetch:
    stealth_mode: true
    js_rendering: true
    captcha_solving: false

search:
  # AI-powered
  exa:
    search_type: "neural"
    max_results: 20
    include_domains: []
    exclude_domains: []
    
  tavily:
    search_depth: "advanced"
    include_answer: true
    include_raw_content: false
    max_results: 10
    
  perplexity:
    model: "sonar-small-online"
    return_citations: true
    return_images: false
  
  # Traditional
  kagi_search:
    safe_search: "moderate"
    region: "us-en"
    freshness: "week"
    
  searxng:
    engines: ["google", "bing", "duckduckgo"]
    safe_search: 1
    time_range: ""

content_processing:
  # Document conversion
  vectorize:
    chunking_strategy: "semantic"
    chunk_size: 1000
    chunk_overlap: 200
    output_format: "markdown"
    
  pandoc:
    input_formats: ["html", "pdf", "docx", "csv"]
    output_format: "markdown"
    preserve_structure: true
    
  markdownify:
    clean_output: true
    preserve_links: true
    include_metadata: true
    
  unstructured:
    chunking_strategy: "by_title"
    max_characters: 1500
    combine_under_n_chars: 200
    new_after_n_chars: 2000
    
  # RAG & knowledge management
  needle:
    indexing_strategy: "semantic"
    chunk_size: 512
    similarity_threshold: 0.8
    max_results: 20
    
  chroma:
    collection_name: "intelforge_knowledge"
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    distance_metric: "cosine"
    persist_directory: "/home/kiriti/alpha_projects/intelforge/vault/vectors"
    
  basic_memory:
    graph_storage: "/home/kiriti/alpha_projects/intelforge/vault/knowledge_graph"
    auto_link_threshold: 0.7
    max_entities: 50000
    
  inkeep:
    search_mode: "semantic"
    return_snippets: true
    max_snippet_length: 200

data:
  sqlite:
    database_path: "/home/kiriti/alpha_projects/intelforge/vault/data"
    backup_interval: "1h"
  
  memory:
    persistence: true
    max_entities: 10000

productivity:
  github:
    rate_limit: 5000
    cache_duration: "5m"
```

## ğŸ” Server Status & Management

### Status Checking

```bash
#!/bin/bash
# status.sh - Check MCP server status

echo "ğŸ” IntelForge MCP Server Status"
echo "================================"

# Check active Claude Code servers
if claude mcp list &>/dev/null; then
    echo "âœ… Claude Code MCP servers:"
    claude mcp list
else
    echo "âŒ Claude Code not configured"
fi

# Check individual server health
echo ""
echo "ğŸ¥ Server Health Check:"

# Test basic servers
echo -n "ğŸ”§ Filesystem: "
if npx -y @modelcontextprotocol/server-filesystem --version &>/dev/null; then
    echo "âœ… Available"
else
    echo "âŒ Not available"
fi

echo -n "ğŸ§  Memory: "
if npx -y @modelcontextprotocol/server-memory --version &>/dev/null; then
    echo "âœ… Available"
else
    echo "âŒ Not available"
fi

echo -n "ğŸ•·ï¸ Puppeteer: "
if npx -y @modelcontextprotocol/server-puppeteer --version &>/dev/null; then
    echo "âœ… Available"
else
    echo "âŒ Not available"
fi
```

## ğŸš¨ Troubleshooting

### Common Issues

**1. Server Won't Start**
```bash
# Check Claude Code config
cat ~/.claude/config.json

# Verify server installation
npx -y @modelcontextprotocol/server-puppeteer --help

# Check logs
tail -f logs/mcp_servers.log
```

**2. Permission Issues**
```bash
# Fix filesystem permissions
chmod +x scripts/*.sh

# Update npm permissions
npm config set prefix ~/.npm-global
```

**3. API Key Issues**
```bash
# Set environment variables
export BRAVE_SEARCH_API_KEY="your-key"
export FIRECRAWL_API_KEY="your-key"

# Verify in Claude Code config
```

### Debug Mode

```bash
# Start servers in debug mode
DEBUG=mcp:* ./scripts/start_servers.sh

# Check specific server logs
tail -f logs/puppeteer.log
```

## ğŸ“ˆ Performance Optimization

### Resource Management

```yaml
# Performance settings
performance:
  concurrent_requests: 5
  timeout: 30000
  memory_limit: "512MB"
  
  scraping:
    delay_between_requests: 1000
    max_retries: 3
    cache_responses: true
    
  data:
    batch_size: 100
    index_frequency: "5m"
```

### Monitoring

```bash
# Monitor resource usage
htop -p $(pgrep -f "mcp-server")

# Check memory usage
ps aux | grep mcp-server | awk '{print $4, $11}'

# Log rotation
logrotate -f /etc/logrotate.d/mcp-servers
```

## ğŸ”® Future Enhancements

### Current Coverage Status

âœ… **Phase 1: Essential Scraping** - 15+ servers implemented  
âœ… **Phase 2: Enterprise & Anti-Detection** - AgentQL, Apify, Rquest, etc.  
âœ… **Phase 3: AI Search & Discovery** - Exa, Tavily, Perplexity, etc.  
ğŸ”„ **Phase 4: Specialized Integration** - In progress  

### Planned Additions

- **ğŸ¤– LLM Integration**: Local model servers, Claude/OpenAI endpoints
- **ğŸ“Š Financial Data**: Bloomberg, Yahoo Finance, Trading View APIs
- **ğŸ” Advanced Security**: Proxy management, VPN integration, CAPTCHA solving
- **ğŸ“± Social Media**: LinkedIn, Twitter, Instagram specialized scrapers
- **â˜ï¸ Cloud Platform**: Kubernetes deployment, monitoring, scaling

### Integration Roadmap

1. **Phase 1**: Essential scraping servers (âœ… Complete - 6 servers)
2. **Phase 2**: Enterprise & anti-detection (âœ… Complete - 9 additional servers)  
3. **Phase 3**: AI search & discovery (âœ… Complete - 7 additional servers)
4. **Phase 4**: Specialized domains (ğŸ“‹ Planned - Financial, social, cloud)
5. **Phase 5**: Enterprise deployment (ğŸ”„ Infrastructure scaling, monitoring)

---

## ğŸ“š Resources

- **[MCP Official Documentation](https://modelcontextprotocol.io)**
- **[Docker MCP Servers](https://github.com/docker/mcp-servers)**
- **[IntelForge Project Documentation](../README.md)**
- **[Claude Code Integration Guide](../docs/claude_code_integration.md)**

---

**ğŸ¯ Next Steps:**
1. **Essential Setup**: Run `./scripts/install_scraping.sh` for core servers
2. **Enterprise Upgrade**: Install AgentQL, Apify, and Browserbase for production scraping
3. **AI Integration**: Configure Exa, Tavily, and Perplexity for intelligent research
4. **Anti-Detection**: Set up Rquest and Scrapling for protected sites
5. **Performance**: Monitor and optimize with the comprehensive server suite

**ğŸ“Š Current Status:** 
- **30+ MCP servers** available across all categories
- **Enterprise-grade** scraping infrastructure 
- **AI-powered** search and discovery
- **Professional content processing** with automatic chunking
- **Production-ready** for comprehensive web intelligence workflows

**ğŸš€ Capability Overview:**
- **Basic Scraping**: Fetch, Puppeteer, Playwright (âœ…)
- **Enterprise Scraping**: AgentQL, Apify, Firecrawl, Browserbase (âœ…) 
- **Anti-Detection**: Rquest, Scrapling Fetch (âœ…)
- **Content Processing**: Vectorize, Pandoc, Markdownify, Unstructured (âœ…)
- **RAG & Knowledge**: Needle, Chroma, Basic Memory (âœ…)
- **AI Search**: Exa, Tavily, Perplexity (âœ…)
- **Privacy Search**: Kagi, SearXNG (âœ…)
- **Specialized Tools**: Screenshot, RAG processing (âœ…)